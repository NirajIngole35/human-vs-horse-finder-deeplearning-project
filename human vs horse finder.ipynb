{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13db0ec8",
   "metadata": {},
   "source": [
    "# HUMAN Vs HOURASE CLASSIFIER PROJECT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dfe342",
   "metadata": {},
   "outputs": [],
   "source": [
    "Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6bc894c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cb5788",
   "metadata": {},
   "outputs": [],
   "source": [
    "load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61afac04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "local_zip = r\"C:\\Users\\HP\\Desktop\\INTERNSHIP 5 MARCH\\deeplearning ___project\\human vs hourse.zip\"\n",
    "zip_ref = zipfile.ZipFile(local_zip, \"r\")\n",
    "zip_ref.extractall(r\"C:\\Users\\HP\\Desktop\\INTERNSHIP 5 MARCH\\deeplearning ___project\\human vs hourse\")\n",
    "zip_ref.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f70c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "directery with our traning human pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38b6c06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "train_hourse_dir = os.path.join(\n",
    "    r\"C:\\Users\\HP\\Desktop\\INTERNSHIP 5 MARCH\\deeplearning ___project\\human vs hourse.zip\",\n",
    "    \"hourse\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "827a3214",
   "metadata": {},
   "outputs": [],
   "source": [
    "#directery with our traning hourse pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57122a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_human_dir= os.path.join(\n",
    "    r\"C:\\Users\\HP\\Desktop\\INTERNSHIP 5 MARCH\\deeplearning ___project\\human vs hourse.zip\",\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e68b4a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['download (1)gf.jpg', 'download (1)h.jpg', 'download (1)hg.jpg', 'download (1)jh.jpg', 'download.jpg', 'fh.jpg', 'images (1)nf.jpg', 'images.jpg', 'pexels-helena-lopes-1996332.jpg']\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "\n",
    "zip_file_path = r\"C:\\Users\\HP\\Desktop\\INTERNSHIP 5 MARCH\\deeplearning ___project\\human vs hourse.zip\"\n",
    "extracted_folder = r\"C:\\Users\\HP\\Desktop\\INTERNSHIP 5 MARCH\\deeplearning ___project\\human vs hourse\"\n",
    "\n",
    "# Extract the ZIP file\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extracted_folder)\n",
    "\n",
    "# List the files in the extracted folder\n",
    "train_hourse_dir = os.path.join(extracted_folder, \"hourse\")\n",
    "train_hourse_names = os.listdir(train_hourse_dir)\n",
    "print(train_hourse_names[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c02c00ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['depositphotos_245768734-stock-photo-bissau-republic-guinea-bissau-february.jpg', 'download (1).jpg', 'download (2).jpg', 'download (3).jpg', 'download (4).jpg', 'download.jpg', 'Ethiopia-UN0418425.jpg', 'images.jpg']\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "zip_file_path = r\"C:\\Users\\HP\\Desktop\\INTERNSHIP 5 MARCH\\deeplearning ___project\\human vs hourse.zip\"\n",
    "extracted_folder = r\"C:\\Users\\HP\\Desktop\\INTERNSHIP 5 MARCH\\deeplearning ___project\\human vs hourse\"\n",
    "\n",
    "# Extract the ZIP file\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extracted_folder)\n",
    "\n",
    "\n",
    "\n",
    "# List the files in the extracted folder for \"human\"\n",
    "train_human_dir = os.path.join(extracted_folder, \"human\")\n",
    "train_human_names = os.listdir(train_human_dir)\n",
    "print(train_human_names[:10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfb1f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "44b50622",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496128c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameter for graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "58b5754c",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ncols=5\n",
    "nrow=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0922032",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7120c548",
   "metadata": {},
   "outputs": [],
   "source": [
    "#index for iterating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3f34bfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "pic_index=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d770cd",
   "metadata": {},
   "source": [
    "#set size of fig 5x5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "522582b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig=plt.gcf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ed1aa86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.set_size_inches(ncols*5,nrow*5)\n",
    "pic_index+=10\n",
    "next_hourse_pic=[os.path.join(train_hourse_dir,fname)\n",
    "                for fname in train_hourse_names[\n",
    "                    pic_index-10:pic_index\n",
    "                ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0f1067ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "next_human_pic=[os.path.join(train_human_dir,fname)\n",
    "                for fname in train_human_names[\n",
    "                    pic_index-10:pic_index\n",
    "                ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5a698660",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,imag_path in enumerate(next_hourse_pic+next_human_pic):\n",
    "    sa=plt.subplot(nrow,ncols,i+1)\n",
    "    sa.axis(\"off\")\n",
    "    imag=mping.imread(imag_path)\n",
    "    plt.imshow(imag)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c7c3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creat neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d6662940",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    # 1st convolution\n",
    "    tf.keras.layers.Conv2D(16, (3, 3), activation=tf.nn.relu, input_shape=(300, 300, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "\n",
    "    # 2nd convolution\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation=tf.nn.relu),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "\n",
    "    # 3rd convolution\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation=tf.nn.relu),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "\n",
    "    # 4th convolution\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation=tf.nn.relu),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "\n",
    "    # 5th convolution\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation=tf.nn.relu),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "\n",
    "    # Flatten\n",
    "    tf.keras.layers.Flatten(),\n",
    "\n",
    "    # Dense\n",
    "    tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d513d0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ef26b99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cc9f18e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4067eeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "df69a4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen=ImageDataGenerator(rescale=1/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e100a106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "    r\"C:\\Users\\HP\\Desktop\\INTERNSHIP 5 MARCH\\deeplearning ___project\\human vs hourse\",\n",
    "    target_size=(300, 300),\n",
    "    batch_size=128,\n",
    "    class_mode=\"binary\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2681ec25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#traning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cd42cd57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      " 1/10 [==>...........................] - ETA: 35s - loss: 0.7002 - accuracy: 0.4706WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 200 batches). You may need to use the repeat() function when building your dataset.\n",
      "10/10 [==============================] - 5s 170ms/step - loss: 0.7002 - accuracy: 0.4706\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_generator,\n",
    "                    steps_per_epoch=10,\n",
    "                    epochs=20,\n",
    "                    verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e8531cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding horse oe human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "037acca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c54b45b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fab69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import google.colab\n",
    "from google.colab import files\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "for fn in uploaded.keys():\n",
    "    path = '/content/' + fn\n",
    "    img = image.load_img(path, target_size=(300, 300))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    images = np.vstack([x])\n",
    "    classes = model.predict(images, batch_size=10)\n",
    "    print(classes[0])\n",
    "    if classes[0][0] > 0.5:\n",
    "        print(fn + \" is human\")\n",
    "    else:\n",
    "        print(fn + \" is horse\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
